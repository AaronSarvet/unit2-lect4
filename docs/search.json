[
  {
    "objectID": "unit2-lect4.html#agenda",
    "href": "unit2-lect4.html#agenda",
    "title": "Inverse Probability Weighting",
    "section": "Agenda",
    "text": "Agenda\n\nReview from last time\n\nCausal models - three assumptions\nIdentification with the g-formula\nThe parametric g-formula estimator\n\nInverse probability weighting (IPW)\n\nIPW estimators\nOther estimators based on the propensity score\nEvaluating positivity"
  },
  {
    "objectID": "unit2-lect4.html#review",
    "href": "unit2-lect4.html#review",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\n\nConsistency for treatment \\(A\\) with respect to an outcome \\(Y\\) is the following property: \\[\n\\begin{aligned}\nP(Y=Y^a \\mid A=a)=1,\n\\end{aligned}\n\\] for all \\(a\\)."
  },
  {
    "objectID": "unit2-lect4.html#review-1",
    "href": "unit2-lect4.html#review-1",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\n\n(Conditional) Exchangeability for treatment \\(A\\) with respect to an outcome \\(Y\\) is the following property: \\[\n\\begin{aligned}\nY^a \\perp A \\mid L\n\\end{aligned}\n\\] for all \\(a\\)."
  },
  {
    "objectID": "unit2-lect4.html#review-2",
    "href": "unit2-lect4.html#review-2",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\n\n(Conditional) positivity for treatment \\(A\\) with respect to an outcome \\(Y\\) is the following property: \\[\n\\begin{aligned}\nP(A=a \\mid L=l)&gt;0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "unit2-lect4.html#review-3",
    "href": "unit2-lect4.html#review-3",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\nAssumptions hold by design in an ideal randomized controlled trial. Outside of a trial they can fail:\n\nConsistency, \\(Y^a \\neq Y\\) when \\(A=a\\): e.g., LDL cholesterol as exposure\nExchangeability, \\(Y^a \\not\\perp A \\mid L\\): unmeasured confounding, i.e., common causes of treatment and the outcome\nPositivity, \\(P(A=a \\mid L=l)=0\\): No individuals with moderate baseline depression are prescribed Ketamine treatment (comparing Ketamine vs. SSRI)."
  },
  {
    "objectID": "unit2-lect4.html#review-4",
    "href": "unit2-lect4.html#review-4",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\nUnder consistency, exchangeability, positivity:\n\\[\\begin{align*}\n   \\mathbb{E}[Y^a]  \n= &\\mathbb{E}\\Big[\\mathbb{E}[Y \\mid A=a, L]\\Big]\n\\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#review-5",
    "href": "unit2-lect4.html#review-5",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\nUnder consistency, exchangeability, positivity, AND a conditional mean outcome model, e.g.:\n\n\\(\\mathbb{E}[Y \\mid A=a, L=l] = \\beta_0 + \\beta_1a + \\beta_2l+\\beta_3al\\)\n\\(logit(P(Y=1 \\mid A=a, L=l)) = \\theta_0 + \\theta_1a + \\theta_2l+\\theta_3al\\)\n\n\\[\\begin{align*}\n   \\mathbb{E}[Y^a]\n= &\\mathbb{E}\\Big[\\mathbb{E}[Y \\mid A=a, L]\\Big] \\\\\n= &\\mathbb{E}\\Big[\\beta_0 + \\beta_1a + \\beta_2L+\\beta_3aL\\Big]\n\\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#review-6",
    "href": "unit2-lect4.html#review-6",
    "title": "Inverse Probability Weighting",
    "section": "Review",
    "text": "Review\nUnder consistency, exchangeability, positivity, AND a conditional mean outcome model, e.g.:\n\\[\\begin{align*}\n  & \\hat{\\mathbb{E}}\\Big[\\hat\\beta_0 + \\hat\\beta_1a + \\hat\\beta_2L+\\hat\\beta_3aL\\Big] \\\\\n= & \\frac{1}{n}\\sum\\limits_{i=1}^n\\Big\\{\\beta_0 + \\hat\\beta_1a + \\hat\\beta_2L_i+\\hat\\beta_3aL_i\\Big\\}\n\\end{align*}\\]\nis a consistent estimator of \\(\\mathbb{E}[Y^{a}]\\)."
  },
  {
    "objectID": "unit2-lect4.html#parametric-g-formula-estimator",
    "href": "unit2-lect4.html#parametric-g-formula-estimator",
    "title": "Inverse Probability Weighting",
    "section": "Parametric g-formula estimator",
    "text": "Parametric g-formula estimator\nParametric g-formula was historically the first estimator proposed for the g-formula parameter. But here is a puzzle: suppose we have data from a special conditionally randomized trial, where we assign patients treatment randomly conditional on a value of a continuous time-varying covariate, for example according to some function \\(\\pi(l)\\). Suppose further that we no nothing a priori about \\(\\mathbb{E}[Y \\mid A=a, L=l]\\), e.g., we have no parametric model for \\(\\mathbb{E}[Y \\mid A=a, L=l]\\). As such, we cannot consistently estimate \\(\\mathbb{E}[Y \\mid A=a, L=l]\\) or apparently the ATE…This seems strange…"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting",
    "href": "unit2-lect4.html#inverse-probability-weighting",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nHow can we use the propensities? Let’s manipulate the g-formula…\n\\[\\begin{align*}\n= & \\mathbb{E}\\Big[\\mathbb{E}[Y \\mid A=a, L]\\Big] \\\\\n= & \\int_l\\int_yyf(y \\mid a, l) f(l) \\\\\n=& \\int_l\\int_y\\int_{a'}y\\frac{I(a'=a)}{f(a' \\mid l)}f(y \\mid a', l) f(a' \\mid l)f(l) \\\\\n=& \\mathbb{E}\\Big[Y \\frac{I(A=a)}{P(A=a \\mid L)}\\Big]\n  \\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-1",
    "href": "unit2-lect4.html#inverse-probability-weighting-1",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nSo, under consistency, exchangeability, positivity…\n\\[\\begin{align*}\n\\mathbb{E}[Y^a] = & \\mathbb{E}\\Big[Y \\frac{I(A=a)}{P(A=a \\mid L)}\\Big]= & \\mathbb{E}\\Big[Y \\frac{1}{P(A=a \\mid L)} \\mid A=a \\Big]\n  \\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-2",
    "href": "unit2-lect4.html#inverse-probability-weighting-2",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nReturning to our motivating example:\n\n“…we assign patients treatment randomly conditional on a value of a continuous time-varying covariate, for example according to some function \\(\\pi(l)\\)…”\n\\[\\begin{align*}\n\\mathbb{E}[Y^{a=1}] = & \\mathbb{E}\\Big[Y \\frac{I(A=1)}{P(A=1 \\mid L)}\\Big] \\\\\n= & \\mathbb{E}\\Big[Y \\frac{I(A=1)}{\\pi(L)}\\Big]\n\\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-3",
    "href": "unit2-lect4.html#inverse-probability-weighting-3",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nUnder consistency, exchangeability, positivity, AND a knowledge that \\(P(A=1 \\mid L=l) = \\pi(l)\\):\n\\[\\begin{align*}\n  = & \\frac{1}{n}\\sum\\limits_{i=1}^nY_i\\frac{I(A_i=1)}{\\pi(L_i)} \\\\\n  = & \\frac{1}{n}\\sum\\limits_{i=1}^nY_iW_i\n\\end{align*}\\]\nis a consistent estimator of \\(\\mathbb{E}[Y^{a=1}]\\), where:\n\\[W_i := \\frac{I(A_i=1)}{\\pi(L_i)}.\\]\n\nThis is called the Horvitz-Thompson estimator."
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-4",
    "href": "unit2-lect4.html#inverse-probability-weighting-4",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\nexpit &lt;- function(x) { exp(x) / (1 + exp(x)) }\nn&lt;-1000000\nbeta&lt;-c(-1, 1, 0.5)\neta&lt;-c(0, 0.2)\n\nU_L &lt;- runif(n, 0, 1) \nU_A &lt;- runif(n, 0, 1) \nU_Y1 &lt;- runif(n, 0, 1)\nU_Y0 &lt;- runif(n, 0, 1)\n\nL&lt;-qnorm(U_L, 2, 1)\nA  &lt;- qbinom(U_A, 1, expit(eta[1] + eta[2]*L))\nY1 &lt;- qnorm(U_Y1, beta[1] + beta[2]*1 + beta[3]*L, 1)\nY0 &lt;- qnorm(U_Y0, beta[1] + beta[2]*0 + beta[3]*L, 1)\nY  &lt;- Y1*A + Y0*(1-A)\n\n df1&lt;-data.frame(L=L, A=A, Y1=Y1, Y0=Y0, Y=Y)"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-5",
    "href": "unit2-lect4.html#inverse-probability-weighting-5",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\n#Constructing weights for a=1\ndf1$pi&lt;-expit(eta[1] + eta[2]*df1$L)\ndf1$W&lt;-df1$A/df1$pi\n\n#Truth\nmean(df1$Y1)\n\n[1] 0.9991558\n\n#Horvitz-Thompson Estimator\nmean(df1$Y*df1$W)\n\n[1] 0.9981318"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-6",
    "href": "unit2-lect4.html#inverse-probability-weighting-6",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nUnder consistency, exchangeability, positivity, AND a knowledge that \\(P(A=1 \\mid L=l) = \\pi(l)\\):\n\\[\\begin{align*}\n  = & \\frac{\\frac{1}{n}\\sum\\limits_{i=1}^nY_i\\frac{I(A_i=1)}{\\pi(L_i)}}{\\frac{1}{n}\\sum\\limits_{i=1}^n\\frac{I(A_i=1)}{\\pi(L_i)}}\n\\end{align*}\\]\nis a consistent estimator estimator of \\(\\mathbb{E}[Y^{a=1}]\\).\n\nThis is called the Hajek estimator."
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-7",
    "href": "unit2-lect4.html#inverse-probability-weighting-7",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\n#Constructing weights for a=1\ndf1$pi&lt;-expit(eta[1] + eta[2]*df1$L)\ndf1$W&lt;-df1$A/df1$pi\n\n#Truth\nmean(df1$Y1)\n\n[1] 0.9991558\n\n#Horvitz-Thompson Estimator\nmean(df1$Y*df1$W)\n\n[1] 0.9981318\n\n#Hajek Estimator\nmean(df1$Y*df1$W)/mean(df1$W)\n\n[1] 0.9987243"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-8",
    "href": "unit2-lect4.html#inverse-probability-weighting-8",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nWhat if we do not know the function, \\(\\pi(l)\\), for example, if we do not have access to data from a conditionally randomized clinical trial, but rather have observational data, which we assume can be viewed as such a trial run by nature?"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-9",
    "href": "unit2-lect4.html#inverse-probability-weighting-9",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nUnder consistency, exchangeability, positivity, AND a conditional treatment model, e.g.:\n\n\\(logit(P(A=1 \\mid L=l)) = \\eta_0 + \\eta_1l\\)\n\n\\[\\begin{align*}\n   \\mathbb{E}[Y^{a=1}]\n= &\\mathbb{E}\\Big[Y \\frac{I(A=1)}{P(A=1 \\mid L)}\\Big] \\\\\n= &\\mathbb{E}\\Big[Y \\frac{I(A=1)}{expit(\\eta_0 + \\eta_1L)}\\Big]\n\\end{align*}\\]"
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-10",
    "href": "unit2-lect4.html#inverse-probability-weighting-10",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\nUnder consistency, exchangeability, positivity, AND a conditional treatment model,\n\\[\\begin{align*}\n= & \\frac{1}{n}\\sum\\limits_{i=1}^n\\Big\\{Y_i\\frac{I(A_i=1)}{expit(\\hat\\eta_0 + \\hat\\eta_1L_i)}\\Big\\}\n\\end{align*}\\]\nis a consistent estimator of \\(\\mathbb{E}[Y^{a=1}]\\)."
  },
  {
    "objectID": "unit2-lect4.html#inverse-probability-weighting-11",
    "href": "unit2-lect4.html#inverse-probability-weighting-11",
    "title": "Inverse Probability Weighting",
    "section": "Inverse probability weighting",
    "text": "Inverse probability weighting\n\n#Constructing weights for a=1\nfit&lt;-glm(A~L, data=df1, family=binomial(link=logit))\ndf1$pi&lt;-predict(fit, df1, type=\"response\")\ndf1$W&lt;-df1$A/df1$pi\n\n#Truth\nmean(df1$Y1)\n\n[1] 0.9991558\n\n#Horvitz-Thompson Estimator\nmean(df1$Y*df1$W)\n\n[1] 0.9990111\n\n#Hajek Estimator\nmean(df1$Y*df1$W)/mean(df1$W)\n\n[1] 0.9990011"
  },
  {
    "objectID": "unit2-lect4.html#test",
    "href": "unit2-lect4.html#test",
    "title": "Inverse Probability Weighting",
    "section": "Test",
    "text": "Test"
  }
]