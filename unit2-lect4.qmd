---
title: "Inverse Probability Weighting"

format:
  revealjs:
    slide-level: 2
    incremental: true
    slide-number: true
    controls: true
    autoScale: true
    minScale: 0.6
    maxScale: 1.0
    center: false
    margin: 0.05
    css: styles.css

    html-math-method:
      method: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

    mathjax:
      macros:
        independent: "\\perp\\!\\!\\perp"
---
## Agenda

- Review from last time
  - Causal models - three assumptions
  - Identification with the g-formula
  - The parametric g-formula estimator
- Inverse probability weighting (IPW)
  - IPW estimators
  - Other estimators based on the propensity score
  - Evaluating positivity

## Review

::: {.definition data-title="Consistency"}
Consistency for treatment $A$ with respect to an outcome $Y$ is the following property:
    $$
\begin{aligned}
P(Y=Y^a \mid A=a)=1,
\end{aligned}
$$
    for all $a$. 
:::

## Review

::: {.definition data-title="(Conditional) Exchangeability"}
(Conditional) Exchangeability for treatment $A$ with respect to an outcome $Y$ is the following property:
    $$
\begin{aligned}
Y^a \perp A \mid L
\end{aligned}
$$
    for all $a$.
:::

## Review
::: {.definition data-title="(Conditional) Positivity"}
(Conditional) positivity for treatment $A$ with respect to an outcome $Y$ is the following property:
$$\begin{align}
P(A=a \mid L=l)>0
\end{align}
$$
    for all $a$, and for all $l$ such that $P(L=l)>0$
:::


## Review

Assumptions hold by design in an ideal randomized controlled trial. Outside of a trial they can fail:

- Consistency, $Y^a \neq Y$ when $A=a$
  - e.g., LDL cholesterol as exposure
- Exchangeability, $Y^a \not\perp A \mid L$
  - unmeasured confounding, i.e., common causes of treatment and the outcome
- Positivity, $P(A=a \mid L=l)=0$
  - No individuals with moderate baseline depression are prescribed Ketamine treatment (comparing Ketamine vs. SSRI).

## Review

Under consistency, exchangeability, positivity:

$$\begin{align*}
   \mathbb{E}[Y^a]  
= &\mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big]
\end{align*}$$

## Review

Under consistency, exchangeability, positivity, AND **a conditional mean outcome model**, e.g.:

 - $\mathbb{E}[Y \mid A=a, L=l] = \beta_0 + \beta_1a + \beta_2l+\beta_3al$
 - $logit(P(Y=1 \mid A=a, L=l)) = \theta_0 + \theta_1a + \theta_2l+\theta_3al$

<p class="fragment"> 
$$\begin{align*}
   \mathbb{E}[Y^a] 
= &\mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big] \\ 
= &\mathbb{E}\Big[\beta_0 + \beta_1a + \beta_2L+\beta_3aL\Big] 
\end{align*}$$
</p>

## Review

Under consistency, exchangeability, positivity, AND a conditional mean outcome model, e.g.:

$$\begin{align*}
  & \hat{\mathbb{E}}\Big[\hat\beta_0 + \hat\beta_1a + \hat\beta_2L+\hat\beta_3aL\Big] \\ 
= & \frac{1}{n}\sum\limits_{i=1}^n\Big\{\beta_0 + \hat\beta_1a + \hat\beta_2L_i+\hat\beta_3aL_i\Big\}
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a}]$.

# Review

Procedure for computing $\hat{\mathbb{E}}[Y^{a}]$:

 - **Step 0:** Regress $Y$ on $A$ and $L$ in the full dataset.
 - **Step 1:** Make a copy of the dataset and set $A=a$
 - **Step 2:** Predict the outcome for each individual using the outcome regression fit, on the scale of the outcome (e.g., probabilities for binary outcomes)
 - **Step 3:** Take the simple average of the predicted outcomes in this dataset
 


## Parametric g-formula estimator {.smaller}

- Parametric g-formula was historically the first estimator proposed for the g-formula parameter. 
- But here is a puzzle
  - suppose we have data from a special conditionally randomized trial
  - therein, we assign patients treatment randomly conditional on a value of a continuous  covariate, for example according to some function $\pi(l)$. 
  - Suppose further that we no nothing a priori about $\mathbb{E}[Y \mid A=a, L=l]$, e.g., we have no parametric model for $\mathbb{E}[Y \mid A=a, L=l]$. 
  - As such, we cannot consistently estimate $\mathbb{E}[Y \mid A=a, L=l]$ or apparently the ATE...This seems strange...

## Inverse probability weighting

How can we use the propensities? Let's manipulate the g-formula...

<p class="fragment">
$$\begin{align*}
= & \mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big] \\ 
= & \int_l\int_yyf(y \mid a, l) f(l) \\
=& \int_l\int_y\int_{a'}y\frac{I(a'=a)}{f(a' \mid l)}f(y \mid a', l) f(a' \mid l)f(l)\end{align*}$$
</p>

<p class="fragment">
$$= \mathbb{E}\Big[Y \frac{I(A=a)}{P(A=a \mid L)}\Big]
  $$
</p>
  
## Inverse probability weighting

So, under consistency, exchangeability, positivity...

$$\begin{align*}
\mathbb{E}[Y^a] = & \mathbb{E}\Big[Y \frac{I(A=a)}{P(A=a \mid L)}\Big]=  \mathbb{E}\Big[Y \frac{1}{P(A=a \mid L)} \mid A=a \Big]
  \end{align*}$$
  
## Inverse probability weighting

Returning to our motivating example:

- "...we assign patients treatment randomly conditional on a value of a continuous  covariate, for example according to some function $\pi(l)$..."
- $$\begin{align*}
\mathbb{E}[Y^{a=1}] = & \mathbb{E}\Big[Y \frac{I(A=1)}{P(A=1 \mid L)}\Big] \\
= & \mathbb{E}\Big[Y \frac{I(A=1)}{\pi(L)}\Big] 
  \end{align*}$$
  
  
## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a knowledge that $P(A=1 \mid L=l) = \pi(l)$:
<p class="fragment">
$$\begin{align*}
  = & \frac{1}{n}\sum\limits_{i=1}^nY_i\frac{I(A_i=1)}{\pi(L_i)} \\ 
  = & \frac{1}{n}\sum\limits_{i=1}^nY_iW_i
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a=1}]$,
</p>

## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a knowledge that $P(A=1 \mid L=l) = \pi(l)$:

<p class="fragment">
I.e., $$ \frac{1}{n}\sum\limits_{i=1}^nY_iW_i \to \mathbb{E}[Y^{a=1}]$$
where:

$$W_i := \frac{I(A_i=1)}{\pi(L_i)}$$
</p>
- This is called the Horvitz-Thompson estimator.

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true
expit <- function(x) { exp(x) / (1 + exp(x)) }
n<-1000
beta<-c(-1, 1, 0.5)
eta<-c(0, 0.2)

U_L <- runif(n, 0, 1) 
U_A <- runif(n, 0, 1) 
U_Y1 <- runif(n, 0, 1)
U_Y0 <- runif(n, 0, 1)

L<-qnorm(U_L, 2, 1)
A  <- qbinom(U_A, 1, expit(eta[1] + eta[2]*L))
Y1 <- qnorm(U_Y1, beta[1] + beta[2]*1 + beta[3]*L, 1)
Y0 <- qnorm(U_Y0, beta[1] + beta[2]*0 + beta[3]*L, 1)
Y  <- Y1*A + Y0*(1-A)

 df1<-data.frame(L=L, A=A, Y1=Y1, Y0=Y0, Y=Y)
```

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
df1$pi<-expit(eta[1] + eta[2]*df1$L)
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)
```





## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a knowledge that $P(A=1 \mid L=l) = \pi(l)$:

$$\begin{align*}
  = & \frac{\frac{1}{n}\sum\limits_{i=1}^nY_i\frac{I(A_i=1)}{\pi(L_i)}}{\frac{1}{n}\sum\limits_{i=1}^n\frac{I(A_i=1)}{\pi(L_i)}}
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a=1}]$.

- This is called the Hajek estimator.


## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
df1$pi<-expit(eta[1] + eta[2]*df1$L)
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)

#Hajek Estimator
mean(df1$Y*df1$W)/mean(df1$W)
```

## Inverse probability weighting

What if we do not know the function, $\pi(l)$, for example, if we do not have access to data from a conditionally randomized clinical trial, but rather have observational data, which we assume can be viewed as such a trial *run by nature*?

## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a conditional treatment model, e.g.:

 - $logit(P(A=1 \mid L=l)) = \eta_0 + \eta_1l$

<p class="fragment">
$$\begin{align*}
   \mathbb{E}[Y^{a=1}] 
= &\mathbb{E}\Big[Y \frac{I(A=1)}{P(A=1 \mid L)}\Big] \\ 
= &\mathbb{E}\Big[Y \frac{I(A=1)}{expit(\eta_0 + \eta_1L)}\Big] 
\end{align*}$$
</p>
## Inverse probability weighting
Under consistency, exchangeability, positivity, AND a conditional treatment model,
 
$$\begin{align*}
= & \frac{1}{n}\sum\limits_{i=1}^n\Big\{Y_i\frac{I(A_i=1)}{expit(\hat\eta_0 + \hat\eta_1L_i)}\Big\}
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a=1}]$.

# Review

Procedure for computing Horvitz-Thompson estimator, $\hat{\mathbb{E}}[Y^{a}]$:

 - **Step 0:** Regress $A$ and $L$ in the full dataset.
 - **Step 1:** Predict the treatment for each individual using the outcome regression fit, on the scale of the outcome (e.g., probabilities for binary outcomes), $\hat P(A=1 \mid L_i)$.
  - **Step 1:** Construct weights, $W_i=\frac{I(A_i=1)}{\hat P(A=1 \mid L_i)}$ if $a=1$, or  $W_i=\frac{I(A_i=0)}{1-\hat P(A=1 \mid L_i)}$ if $a=0$. 
 - **Step 3:** Take the weighted average of the outcomes.

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
fit<-glm(A~L, data=df1, family=binomial(link=logit))
df1$pi<-predict(fit, df1, type="response")
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)

#Hajek Estimator
mean(df1$Y*df1$W)/mean(df1$W)
```

## Inverse probability weighting

Returning to our motivating example:

- "...we assign patients treatment randomly conditional on a value of a continuous  covariate, for example according to some function $\pi(l)$..."

- Suppose $\pi(l) = expit(\eta_0 + \eta_1L)$.
- Consider:
  - $W^{true} = \frac{I(A=1)}{expit(\eta_0 + \eta_1L)}$
  - $W^{est} = \frac{I(A=1)}{expit(\hat{\eta}_0 + \hat{\eta}_1L)}$
- Which weights should we use?

## Evaluating the positivity assumption

::: {.definition data-title="(Conditional) Positivity"}
(Conditional) positivity for treatment $A$ with respect to an outcome $Y$ is the following property:
$$\begin{aligned}
P(A=a \mid L=l)>0
\end{aligned}$$
    for all $a$, and for all $l$ such that $P(L=l)>0$
:::

## Evaluating the positivity assumption

Equivalent definitions of (Conditional) Positivity:

- Letting $\pi(L) := P(A=1 \mid L)$...
  - $P(0<\pi(L)<1) = 1$
    - Interpretation: the propensity is **never** 0 or 1 for any level of covariates that can occur in the population.
  -  $f(\pi(L) \mid A=1)>0 \iff f(\pi(L) \mid A=0)>0$
    - Interpretation: There must be "overlap" in the propensity score distributions between treated and control populations. 
  
## Evaluating the positivity assumption

Assessing "overlap", $f(\pi(L) \mid A=1)>0 \iff f(\pi(L) \mid A=0)>0$


```{r}

library(ggplot2)

ggplot(df1, aes(x = pi, fill = factor(A), color = factor(A))) +
  geom_density(alpha = 0.3, linewidth = 1) +
  scale_color_manual(values = c("#2C7BB6", "#D7191C")) +
  scale_fill_manual(values = c("#2C7BB6", "#D7191C")) +
  labs(
    x = "Propensity Score",
    y = "Density",
    color = "",
    fill = "",
    title = "Propensity Score Overlap"
  ) +
  theme_minimal(base_size = 14)
```

## Evaluating the positivity assumption

Changing the treatment mechanism...
```{r}
#| echo: true
#| eval: true
#| 
n <- 10000
beta <- c(-1, 1, 0.5)

eta2 <- c(0, 4)

U_L <- runif(n, 0, 1)
U_A <- runif(n, 0, 1)
U_Y1 <- runif(n, 0, 1)
U_Y0 <- runif(n, 0, 1)

L  <- qnorm(U_L, 0, 1)

A2 <- qbinom(U_A, 1, expit(eta2[1] + eta2[2]*L))

Y1 <- qnorm(U_Y1, beta[1] + beta[2]*1 + beta[3]*L, 1)
Y0 <- qnorm(U_Y0, beta[1] + beta[2]*0 + beta[3]*L, 1)

Y2 <- Y1*A2 + Y0*(1-A2)
```

## Evaluating the positivity assumption

Changing the treatment mechanism...
```{r}
#| echo: true
#| eval: true
#| 
df2 <- data.frame(L=L, A=A2, Y1=Y1, Y0=Y0, Y=Y2)

fit2<-glm(A~L, data=df2, family=binomial(link=logit))
df2$pi<-predict(fit2, df2, type="response")
```

## Evaluating the positivity assumption

Assessing "overlap", $f(\pi(L) \mid A=1)>0 \iff f(\pi(L) \mid A=0)>0$

```{r}

ggplot(df2, aes(x = pi, fill = factor(A), color = factor(A))) +
  geom_density(alpha = 0.3, linewidth = 1) +
  scale_color_manual(values = c("#2C7BB6", "#D7191C")) +
  scale_fill_manual(values = c("#2C7BB6", "#D7191C")) +
  labs(
    x = "Propensity Score",
    y = "Density",
    color = "",
    fill = "",
    title = "Propensity Score Overlap"
  ) +
  theme_minimal(base_size = 14)
```


## What to do when there are positivity violations?

When $P(A=a \mid L=l)=0$ for some levels $l$, the IPW estimator 

$$\sum\limits_{i=1}^nY_i\frac{I(A_i=a)}{\hat{P}(A=a \mid L_i)}$$ is not consistent for $\mathbb{E}[Y^a]$.

Rather it is consistent for $$\mathbb{E}\Big[Y^a \mid L \in \{l \ s.t. P(A=a \mid L=l)>0\}\Big]$$


## What to do when there are positivity violations?

As such: $$\Bigg\{\sum\limits_{i=1}^nY_i\frac{I(A_i=1)}{\hat{P}(A=1 \mid L_i)}\Bigg\} - \Bigg\{\sum\limits_{i=1}^nY_i\frac{I(A_i=0)}{\hat{P}(A=0 \mid L_i)}\Bigg\}$$ is not consistent for $\mathbb{E}[Y^{a=1}] - \mathbb{E}[Y^{a=0}]$. 

## What to do when there are positivity violations?

Instead it is consistent for 
$$\begin{align} & \mathbb{E}\Big[Y^{a=1} \mid L \in \{l \ s.t. P(A=1 \mid L=l)>0\}\Big] \\ - & \mathbb{E}\Big[Y^{a=0} \mid L \in \{l \ s.t. P(A=0 \mid L=l)>0\}\Big]\end{align}$$

The subpopulations are different, so it is not even a causal effect of any kind!

## What to do when there are (apparent) positivity violations?

Option 1: the parametric g-formula

- Recall: Positivity ensures the g-formula is well-defined, e.g.,
$$\mathbb{E}\Big[\mathbb{E}[Y \mid A=1, L]\Big]$$
- Suppose $P(A=1 \mid L=1)=0$, binary $L$.
- $$\begin{align}&\mathbb{E}\Big[\mathbb{E}[Y \mid A=1, L]\Big] \\ = & \mathbb{E}[Y \mid A=1, L=1]P(L=1) + \mathbb{E}[Y \mid A=1, L=0]P(L=0)\end{align}$$
- $\mathbb{E}[Y \mid A=1, L=1]$ is the problematic term.

## What to do when there are (apparent) positivity violations?

Option 1: the parametric g-formula

- Suppose we assume $\mathbb{E}[Y \mid A, L] = \beta_0 + \beta_1A + \beta_2L$.
- Consider: 
$$\begin{align} \mathbb{E}[Y \mid A=1, L=1] = \beta_0 + \beta_1 + \beta_2
\end{align}$$ 
- $\beta_0 = \mathbb{E}[Y \mid A=0, L=0]$
- $\beta_1 = \mathbb{E}[Y \mid A=1, L=l] - \mathbb{E}[Y \mid A=0, L=l]$
- $\beta_2 = \mathbb{E}[Y \mid A=a, L=1] - \mathbb{E}[Y \mid A=a, L=0]$


## What to do when there are (apparent) positivity violations?

Option 1: the parametric g-formula

- Suppose we assume $\mathbb{E}[Y \mid A, L] = \beta_0 + \beta_1A + \beta_2L$.
- Consider: 
$$\begin{align} \mathbb{E}[Y \mid A=1, L=1] = \beta_0 + \beta_1 + \beta_2
\end{align}$$ 
- $\beta_0 = \mathbb{E}[Y \mid A=0, L=0]$
- $\beta_1 = \mathbb{E}[Y \mid A=1, L=0] - \mathbb{E}[Y \mid A=0, L=0]$
- $\beta_2 = \mathbb{E}[Y \mid A=0, L=1] - \mathbb{E}[Y \mid A=0, L=0]$
- Warning: We rely on potential wrong modelling assumptions.

## What to do when there are (apparent) positivity violations?

Option 2: Change the estimand

- Originally we wanted $\mathbb{E}[Y^{a=1}] - \mathbb{E}[Y^{a=0}]$.
  - This requires $P(A=a \mid L=l)>1$ for all $l$ with positive probability in the target population.
- Suppose instead we wanted $\mathbb{E}[Y^{a=1} \mid L=l'] - \mathbb{E}[Y^{a=0}\mid L=l']$.
  - $\mathbb{E}[Y^{a} \mid L=l'] = \mathbb{E}[Y \mid A=a, L=l']$
  - This only requires $P(A=a \mid L=l')>0$.

## What to do when there are (apparent) positivity violations?

Option 2: Change the estimand

- Suppose instead we wanted a $\mathbb{E}[Y^{a=1} \mid X=x] - \mathbb{E}[Y^{a=0}\mid X=x]$. 
  - $\mathbb{E}[Y^{a} \mid X=x] = \mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L] \Big | X=x\Big]$
  - This only requires $P(A=a \mid L=l)>0$ for all $l$ with positive probability in the *subpopulation* with $X=x$.
  
## What to do when there are (apparent) positivity violations?  

Option 2: Change the estimand

- Exclude the population you know has probability $0$ of receiving one of the treatment levels, e.g.,
  - $L=(L_1, L_2)$, exclude people with $L_2=0$, moderate depression (ketamine vs. SSRI)
  - $\mathbb{E}[Y^{a} \mid L_2\neq 0] = \mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L_1, L_2] \Big |  L_2\neq 0\Big]$

## What to do when there are (apparent) positivity violations?  

Option 2: Change the estimand

- Restrict to the "overlap population"
  - Only levels of $L$ such that $\epsilon<P(A=1 \mid L=l)<1-\epsilon$
  - $\mathbb{E}\Big[Y^{a} \Big |  L \in \{l \ s.t. \epsilon<P(A=a \mid L=l)<1-\epsilon\}\Big]$ for $0<\epsilon<1$.
  
  
## What to do when there are (apparent) positivity violations?  

```{r}

ggplot(df2, aes(x = pi, fill = factor(A), color = factor(A))) +
  geom_density(alpha = 0.3, linewidth = 1) +
  scale_color_manual(values = c("#2C7BB6", "#D7191C")) +
  scale_fill_manual(values = c("#2C7BB6", "#D7191C")) +
  labs(
    x = "Propensity Score",
    y = "Density",
    color = "",
    fill = "",
    title = "Propensity Score Overlap"
  ) +
  theme_minimal(base_size = 14)
``` 
  
## What to do when there are (apparent) positivity violations?  

```{r}

df3<-df2[df2$pi>0.2 & df2$pi<0.8 , ]
ggplot(df3, aes(x = pi, fill = factor(A), color = factor(A))) +
  geom_density(alpha = 0.3, linewidth = 1) +
  scale_color_manual(values = c("#2C7BB6", "#D7191C")) +
  scale_fill_manual(values = c("#2C7BB6", "#D7191C")) +
  labs(
    x = "Propensity Score",
    y = "Density",
    color = "",
    fill = "",
    title = "Propensity Score Overlap"
  ) +
  theme_minimal(base_size = 14)
``` 
## What to do when there are (apparent) positivity violations?  

Warning: we need to be really careful when we change the estimand! Are we really interested in this subpopulation? What is the policy relavence? Beware of easy fixes - there is no free lunch...



  
  




