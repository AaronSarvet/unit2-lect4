---
title: "Inverse Probability Weighting"

format:
  revealjs:
    slide-level: 2
    incremental: true
    slide-number: true
    controls: true
    autoScale: true
    minScale: 0.6
    maxScale: 1.0
    center: false
    margin: 0.05
    css: styles.css

    html-math-method:
      method: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

    mathjax:
      macros:
        independent: "\\perp\\!\\!\\perp"
---
## Agenda

- Review from last time
  - Causal models - three assumptions
  - Identification with the g-formula
  - The parametric g-formula estimator
- Inverse probability weighting (IPW)
  - IPW estimators
  - Other estimators based on the propensity score
  - Evaluating positivity

## Review

::: {.definition data-title="Consistency"}
Consistency for treatment $A$ with respect to an outcome $Y$ is the following property:
    $$
\begin{aligned}
P(Y=Y^a \mid A=a)=1,
\end{aligned}
$$
    for all $a$. 
:::

## Review

::: {.definition data-title="(Conditional) Exchangeability"}
(Conditional) Exchangeability for treatment $A$ with respect to an outcome $Y$ is the following property:
    $$
\begin{aligned}
Y^a \perp A \mid L
\end{aligned}
$$
    for all $a$.
:::

## Review
::: {.definition data-title="(Conditional) Positivity"}
(Conditional) positivity for treatment $A$ with respect to an outcome $Y$ is the following property:
    $$
\begin{aligned}
P(A=a \mid L=l)>0
\end{aligned}
$$

::: 

## Review

Assumptions hold by design in an ideal randomized controlled trial. Outside of a trial they can fail:

- Consistency, $Y^a \neq Y$ when $A=a$: e.g., LDL cholesterol as exposure
- Exchangeability, $Y^a \not\perp A \mid L$: unmeasured confounding, i.e., common causes of treatment and the outcome
- Positivity, $P(A=a \mid L=l)=0$: No individuals with moderate baseline depression are prescribed Ketamine treatment (comparing Ketamine vs. SSRI).

## Review

Under consistency, exchangeability, positivity:

$$\begin{align*}
   \mathbb{E}[Y^a]  
= &\mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big]
\end{align*}$$

## Review

Under consistency, exchangeability, positivity, AND a conditional mean outcome model, e.g.:

 - $\mathbb{E}[Y \mid A=a, L=l] = \beta_0 + \beta_1a + \beta_2l+\beta_3al$
 - $logit(P(Y=1 \mid A=a, L=l)) = \theta_0 + \theta_1a + \theta_2l+\theta_3al$

$$\begin{align*}
   \mathbb{E}[Y^a] 
= &\mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big] \\ 
= &\mathbb{E}\Big[\beta_0 + \beta_1a + \beta_2L+\beta_3aL\Big] 
\end{align*}$$

## Review

Under consistency, exchangeability, positivity, AND a conditional mean outcome model, e.g.:

$$\begin{align*}
  & \hat{\mathbb{E}}\Big[\hat\beta_0 + \hat\beta_1a + \hat\beta_2L+\hat\beta_3aL\Big] \\ 
= & \frac{1}{n}\sum\limits_{i=1}^n\Big\{\beta_0 + \hat\beta_1a + \hat\beta_2L_i+\hat\beta_3aL_i\Big\}
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a}]$.

# Review

Procedure for computing $\hat{\mathbb{E}}[Y^{a}]$:

 - **Step 0:** Regress $Y$ on $A$ and $L$ in the full dataset.
 - **Step 1:** Make a copy of the dataset and set $A=a$
 - **Step 2:** Predict the outcome for each individual using the outcome regression fit, on the scale of the outcome (e.g., probabilities for binary outcomes)
 - **Step 3:** Take the simple average of the predicted outcomes in this dataset
 


## Parametric g-formula estimator {.smaller}

Parametric g-formula was historically the first estimator proposed for the g-formula parameter. But here is a puzzle: suppose we have data from a special conditionally randomized trial, where we assign patients treatment randomly conditional on a value of a continuous time-varying covariate, for example according to some function $\pi(l)$. Suppose further that we no nothing a priori about $\mathbb{E}[Y \mid A=a, L=l]$, e.g., we have no parametric model for $\mathbb{E}[Y \mid A=a, L=l]$. As such, we cannot consistently estimate $\mathbb{E}[Y \mid A=a, L=l]$ or apparently the ATE...This seems strange...

## Inverse probability weighting

How can we use the propensities? Let's manipulate the g-formula...

$$\begin{align*}
= & \mathbb{E}\Big[\mathbb{E}[Y \mid A=a, L]\Big] \\ 
= & \int_l\int_yyf(y \mid a, l) f(l) \\
=& \int_l\int_y\int_{a'}y\frac{I(a'=a)}{f(a' \mid l)}f(y \mid a', l) f(a' \mid l)f(l) \\
=& \mathbb{E}\Big[Y \frac{I(A=a)}{P(A=a \mid L)}\Big]
  \end{align*}$$
  
## Inverse probability weighting

So, under consistency, exchangeability, positivity...

$$\begin{align*}
\mathbb{E}[Y^a] = & \mathbb{E}\Big[Y \frac{I(A=a)}{P(A=a \mid L)}\Big]= & \mathbb{E}\Big[Y \frac{1}{P(A=a \mid L)} \mid A=a \Big]
  \end{align*}$$
  
## Inverse probability weighting

Returning to our motivating example:

- "...we assign patients treatment randomly conditional on a value of a continuous time-varying covariate, for example according to some function $\pi(l)$..."
- $$\begin{align*}
\mathbb{E}[Y^{a=1}] = & \mathbb{E}\Big[Y \frac{I(A=1)}{P(A=1 \mid L)}\Big] \\
= & \mathbb{E}\Big[Y \frac{I(A=1)}{\pi(L)}\Big] 
  \end{align*}$$
  
  
## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a knowledge that $P(A=1 \mid L=l) = \pi(l)$:

$$\begin{align*}
  = & \frac{1}{n}\sum\limits_{i=1}^nY_i\frac{I(A_i=1)}{\pi(L_i)} \\ 
  = & \frac{1}{n}\sum\limits_{i=1}^nY_iW_i
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a=1}]$, where:

$$W_i := \frac{I(A_i=1)}{\pi(L_i)}.$$

- This is called the Horvitz-Thompson estimator.

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true
expit <- function(x) { exp(x) / (1 + exp(x)) }
n<-1000000
beta<-c(-1, 1, 0.5)
eta<-c(0, 0.2)

U_L <- runif(n, 0, 1) 
U_A <- runif(n, 0, 1) 
U_Y1 <- runif(n, 0, 1)
U_Y0 <- runif(n, 0, 1)

L<-qnorm(U_L, 2, 1)
A  <- qbinom(U_A, 1, expit(eta[1] + eta[2]*L))
Y1 <- qnorm(U_Y1, beta[1] + beta[2]*1 + beta[3]*L, 1)
Y0 <- qnorm(U_Y0, beta[1] + beta[2]*0 + beta[3]*L, 1)
Y  <- Y1*A + Y0*(1-A)

 df1<-data.frame(L=L, A=A, Y1=Y1, Y0=Y0, Y=Y)
```

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
df1$pi<-expit(eta[1] + eta[2]*df1$L)
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)

```



## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a knowledge that $P(A=1 \mid L=l) = \pi(l)$:

$$\begin{align*}
  = & \frac{\frac{1}{n}\sum\limits_{i=1}^nY_i\frac{I(A_i=1)}{\pi(L_i)}}{\frac{1}{n}\sum\limits_{i=1}^n\frac{I(A_i=1)}{\pi(L_i)}}
\end{align*}$$

is a consistent estimator estimator of $\mathbb{E}[Y^{a=1}]$.

- This is called the Hajek estimator.


## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
df1$pi<-expit(eta[1] + eta[2]*df1$L)
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)

#Hajek Estimator
mean(df1$Y*df1$W)/mean(df1$W)
```

## Inverse probability weighting

What if we do not know the function, $\pi(l)$, for example, if we do not have access to data from a conditionally randomized clinical trial, but rather have observational data, which we assume can be viewed as such a trial *run by nature*?

## Inverse probability weighting

Under consistency, exchangeability, positivity, AND a conditional treatment model, e.g.:

 - $logit(P(A=1 \mid L=l)) = \eta_0 + \eta_1l$

$$\begin{align*}
   \mathbb{E}[Y^{a=1}] 
= &\mathbb{E}\Big[Y \frac{I(A=1)}{P(A=1 \mid L)}\Big] \\ 
= &\mathbb{E}\Big[Y \frac{I(A=1)}{expit(\eta_0 + \eta_1L)}\Big] 
\end{align*}$$

## Inverse probability weighting
Under consistency, exchangeability, positivity, AND a conditional treatment model,
 
$$\begin{align*}
= & \frac{1}{n}\sum\limits_{i=1}^n\Big\{Y_i\frac{I(A_i=1)}{expit(\hat\eta_0 + \hat\eta_1L_i)}\Big\}
\end{align*}$$

is a consistent estimator of $\mathbb{E}[Y^{a=1}]$.

# Review

Procedure for computing Horvitz-Thompson estimator, $\hat{\mathbb{E}}[Y^{a}]$:

 - **Step 0:** Regress $A$ and $L$ in the full dataset.
 - **Step 1:** Predict the treatment for each individual using the outcome regression fit, on the scale of the outcome (e.g., probabilities for binary outcomes), $\hat P(A=1 \mid L_i)$
  - **Step 1:** Construct weights, $W_i=\frac{I(A_i=1)}{\hat P(A=1 \mid L_i)}$ if $a=1$, or  $W_i=\frac{I(A_i=0)}{1-\hat P(A=1 \mid L_i)}$ if $a=0$. 
 - **Step 3:** Take the weighted average of the outcomes.

## Inverse probability weighting

```{r}
#| echo: true
#| eval: true

#Constructing weights for a=1
fit<-glm(A~L, data=df1, family=binomial(link=logit))
df1$pi<-predict(fit, df1, type="response")
df1$W<-df1$A/df1$pi

#Truth
mean(df1$Y1)

#Horvitz-Thompson Estimator
mean(df1$Y*df1$W)

#Hajek Estimator
mean(df1$Y*df1$W)/mean(df1$W)
```







